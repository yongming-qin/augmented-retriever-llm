{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "class policy_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_config=\"bert-base-uncased\", add_linear=False, embedding_size=128, freeze_encoder=True, context_net=False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_config)\n",
    "        print(\"model_config:\", model_config)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_config)\n",
    "        \n",
    "        # Freeze transformer encoder and only train the linear layer\n",
    "        if freeze_encoder:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if add_linear:\n",
    "            # Add an additional small, adjustable linear layer on top of BERT tuned through RL\n",
    "            self.embedding_size = embedding_size\n",
    "            if context_net:\n",
    "                input_dim = self.model.config.hidden_size * 2\n",
    "            else:\n",
    "                input_dim = self.model.config.hidden_size\n",
    "            self.linear = nn.Linear(input_dim,\n",
    "                                    embedding_size)  # 768 for bert-base-uncased, distilbert-base-uncased\n",
    "        else:\n",
    "            self.linear = None\n",
    "            \n",
    "    def forward(self, input_list, bert_forward=True, linear_forward=True):\n",
    "        if bert_forward:\n",
    "            input = self.tokenizer(input_list, truncation=True, padding=True, return_tensors=\"pt\").to(self.model.device)\n",
    "            # print(f\"input: {input}\")\n",
    "            output = self.model(**input, output_hidden_states=True)\n",
    "            # Get last layer hidden states\n",
    "            last_hidden_states = output.hidden_states[-1]\n",
    "            # Get [CLS] hidden states\n",
    "            sentence_embedding = last_hidden_states[:, 0, :]  # len(input_list) x hidden_size\n",
    "            # print(f\"sentence_embedding: {sentence_embedding}\")\n",
    "\n",
    "        if linear_forward:\n",
    "            if self.linear:\n",
    "                if bert_forward:\n",
    "                    sentence_embedding = self.linear(sentence_embedding)  # len(input_list) x embedding_size\n",
    "                else:\n",
    "                    sentence_embedding = self.linear(input_list)\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = policy_network(add_linear=True,\n",
    "                       freeze_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
