{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "class policy_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_config=\"bert-base-uncased\", add_linear=False, embedding_size=128, freeze_encoder=True, context_net=False):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_config)\n",
    "        print(\"model_config:\", model_config)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_config)\n",
    "        \n",
    "        # Freeze transformer encoder and only train the linear layer\n",
    "        if freeze_encoder:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if add_linear:\n",
    "            # Add an additional small, adjustable linear layer on top of BERT tuned through RL\n",
    "            self.embedding_size = embedding_size\n",
    "            if context_net:\n",
    "                input_dim = self.model.config.hidden_size * 2\n",
    "            else:\n",
    "                input_dim = self.model.config.hidden_size\n",
    "            self.linear = nn.Linear(input_dim,\n",
    "                                    embedding_size)  # 768 for bert-base-uncased, distilbert-base-uncased\n",
    "        else:\n",
    "            self.linear = None\n",
    "            \n",
    "    def forward(self, input_list, bert_forward=True, linear_forward=True):\n",
    "        if bert_forward:\n",
    "            input = self.tokenizer(input_list, truncation=True, padding=True, return_tensors=\"pt\").to(self.model.device)\n",
    "            # print(f\"input: {input}\")\n",
    "            output = self.model(**input, output_hidden_states=True)\n",
    "            # Get last layer hidden states\n",
    "            last_hidden_states = output.hidden_states[-1]\n",
    "            # Get [CLS] hidden states\n",
    "            sentence_embedding = last_hidden_states[:, 0, :]  # len(input_list) x hidden_size\n",
    "            # print(f\"sentence_embedding: {sentence_embedding}\")\n",
    "\n",
    "        if linear_forward:\n",
    "            if self.linear:\n",
    "                if bert_forward:\n",
    "                    sentence_embedding = self.linear(sentence_embedding)  # len(input_list) x embedding_size\n",
    "                else:\n",
    "                    sentence_embedding = self.linear(input_list)\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper implementation is not straightforward since **AutoModelForTokenClassification** already has a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if False:\n",
    "    model = policy_network(add_linear=True,\n",
    "                       freeze_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token  token_id  attention_mask\n",
      "0  [CLS]       101               1\n",
      "1    the      1996               1\n",
      "2     un      4895               1\n",
      "3  ##bel      8671               1\n",
      "4    ##i      2072               1\n",
      "5  [SEP]       102               1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentence = \"The unbelievable quick brown fox jumps over the lazy dog.\"[:10]\n",
    "\n",
    "# Tokenize the sentence\n",
    "encoded = tokenizer(sentence, return_tensors=\"pt\", return_attention_mask=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "attention_mask = encoded.attention_mask[0]\n",
    "\n",
    "# Visualize the tokenized sentence\n",
    "df = pd.DataFrame({\n",
    "    \"token\": tokens,\n",
    "    \"token_id\": token_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "['al', '##b', '##ja', ';', 'f', '##d', '##j', ';', 'ld', '##f', '##j']\n",
      "100\n",
      "[2632, 2497, 3900, 1025, 1042, 2094, 3501, 1025, 25510, 2546, 3501]\n"
     ]
    }
   ],
   "source": [
    "word = \"albja;fdj;ldfj\"\n",
    "print(word in tokenizer.vocab)  # True\n",
    "print(tokenizer.tokenize(word))  # 23653\n",
    "print(tokenizer.convert_tokens_to_ids(word))  # 23653\n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)))  # 23653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           token  token_id  segment_id  attention_mask\n",
      "0          [CLS]       101           0               1\n",
      "1            the      1996           0               1\n",
      "2   unbelievable     23653           0               1\n",
      "3          quick      4248           0               1\n",
      "4          brown      2829           0               1\n",
      "5            fox      4419           0               1\n",
      "6          jumps     14523           0               1\n",
      "7           over      2058           0               1\n",
      "8            the      1996           0               1\n",
      "9           lazy     13971           0               1\n",
      "10           dog      3899           0               1\n",
      "11             .      1012           0               1\n",
      "12         [SEP]       102           0               1\n",
      "13           the      1996           1               1\n",
      "14        orange      4589           1               1\n",
      "15           cat      4937           1               1\n",
      "16         jumps     14523           1               1\n",
      "17          over      2058           1               1\n",
      "18           the      1996           1               1\n",
      "19           dog      3899           1               1\n",
      "20             .      1012           1               1\n",
      "21         [SEP]       102           1               1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentence = \"The unbelievable quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"The orange cat jumps over the dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "encoded = tokenizer(sentence, sentence2, return_tensors=\"pt\", return_attention_mask=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids[0])\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "attention_mask = encoded.attention_mask[0]\n",
    "segment_ids = encoded.token_type_ids[0]\n",
    "\n",
    "# Visualize the tokenized sentence\n",
    "df = pd.DataFrame({\n",
    "    \"token\": tokens,\n",
    "    \"token_id\": token_ids,\n",
    "    \"segment_id\": segment_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: {'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "output: odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "output.last_hidden_state.shape: torch.Size([1, 6, 768])\n",
      "output.pooler_output.shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "input = tokenizer(\"Hello, world!\", return_tensors=\"pt\")\n",
    "print(f\"input: {input}\")\n",
    "output = model(**input)\n",
    "print(f\"output: {output.keys()}\") # output: odict_keys(['last_hidden_state', 'pooler_output'])\n",
    "\n",
    "print(f\"output.last_hidden_state.shape: {output.last_hidden_state.shape}\") # output.last_hidden_state.shape: torch.Size([1, 6, 768])\n",
    "\n",
    "print(f\"output.pooler_output.shape: {output.pooler_output.shape}\") # output.pooler_output.shape: torch.Size([1, 768])\n",
    "\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
